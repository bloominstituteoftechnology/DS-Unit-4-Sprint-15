## Check For Understanding

1. What does the `prompt` parameter in the SDK do?
    - A) Limits the number of tokens in the output
    - B) Sets the initial string to guide the model's output
    - C) Controls the randomness of the output
    - D) Filters the token pool before choosing the next token

2. What value of `temperature` would make the output more deterministic?
    - A) 0.2
    - B) 0.5
    - C) 0.8
    - D) 1.0

3. What does `max_tokens` parameter do?
    - A) Controls randomness
    - B) Sets the maximum length of the output in tokens
    - C) Specifies the engine
    - D) Sets the prompt for the model

4. What is the purpose of the `top_p` parameter?
    - A) Controls temperature
    - B) Manages pagination
    - C) Limits output length
    - D) Controls nucleus sampling

5. What is the default value for `temperature` if not specified?
    - A) 0
    - B) 0.7
    - C) 1
    - D) 0.5

6. How do you extract generated text from a response object?
    - A) `response.text`
    - B) `response.choices[0].text.strip()`
    - C) `response.output.text`
    - D) `response['text']`

7. What does a system prompt typically do?
    - A) Guides the tone and behavior of the model
    - B) Limits the number of tokens
    - C) Controls randomness
    - D) Manages pagination

8. Can you use system prompts to enforce ethical guidelines?
    - A) Yes
    - B) No
    - C) Only for text summarization
    - D) Only for conversational agents

9. Which of the following is a valid role for a system prompt?
   - A) moderator
   - B) guide
   - C) system
   - D) admin

10. The `temperature` parameter has a value range between 0 and 1.  
    - True
    - False

11. Lower `top_p` values make the text more focused and deterministic.
    - True
    - False

12. You cannot use both `max_tokens` and `top_p` in the same API call.
    - True
    - False

13. `max_tokens` can limit the number of tokens in both input and output.
    - True
    - False

14. System prompts are placed at the end of the prompt string.
    - True
    - False

15. System prompts do not consume tokens from the total token count.
    - True
    - False

16. System prompts are only used for conversational directives.
    - True
    - False

17. `max_tokens` cannot be more than 4096 for a Davinci model.
    - True
    - False

18. You can specify more than one role in a single API call.
    - True
    - False

19. `top_p` controls the size of the pool from which the next token is selected.
    - True
    - False

20. Which parameter allows you to adjust the level of creativity in the output?
    - A) prompt
    - B) temperature
    - C) max_tokens
    - D) top_p
