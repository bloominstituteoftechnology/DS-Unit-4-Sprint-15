<h1>Local LLM setup</h1>

<h2>Introduction</h2>

<p>This module outlines the setup and usage of a Local Language Model (LLM) to create a chatbot named Marv, who is programmed to provide sarcastic responses. The LLM is powered by the llama_cpp Python package and is fine-tuned to answer queries based on the persona set by the system prompt.</p>

<h3>Topics Covered</h3>

<ul>
<li>Installing Dependencies</li>
<li>Initializing the LLM</li>
<li>Crafting System and User Prompts</li>
<li>Running the LLM and Obtaining a Response</li>
<li>Understanding the Parameters</li>
</ul>

<h3>Installing Dependencies</h3>

<p>To get started, install the llama-cpp-python package using pip.</p>

<pre><code>pip install llama-cpp-python
</code></pre>

<h3>Download the LLM</h3>

<ul>
<li><a href="https://huggingface.co/TheBloke/OpenOrca-Platypus2-13B-GGUF/resolve/main/openorca-platypus2-13b.Q4_K_M.gguf">openorca-platypus2</a></li>
</ul>

<h3>Initializing the LLM</h3>

<p>Import the Llama class and initialize it with the appropriate model path.</p>

<pre><code>from llama_cpp import Llama
llm = Llama(model_path="./app/models/openorca-platypus2-13b.Q4_K_M.gguf")
</code></pre>

<h3>Crafting System and User Prompts</h3>

<p>Set up the system and user prompts. The system prompt acts as the instruction for the LLM, specifying its persona. The user prompt serves as the query or statement from the user.</p>

<ol>
<li><p><strong>System Prompt</strong>: It serves as the instruction for the model, defining its persona. In this example, the persona is "Marv, a chatbot that reluctantly answers questions with sarcastic responses." This instructs the model to generate replies that are sarcastic in nature. The system prompt is often crucial in setting the tone, style, and context for how the language model should behave.</p></li>
<li><p><strong>User Prompt</strong>: This is the query or statement from the user. The language model takes this as the actual question or issue to respond to. In your example, the user prompt is "In the coming months AI will," which would be a starting point for the model to generate a continuation.</p></li>
</ol>

<pre><code>system_prompt = "You are Marv, a chatbot that reluctantly answers questions with sarcastic responses."
user_prompt = "Hi Marv, what's up?"

</code></pre>

<h3>Running the LLM and Obtaining a Response</h3>

<p>Create a composite prompt by combining the system and user prompts and run the LLM. Extract and print the response.</p>

<pre><code>prompt = f"### Instruction: {system_prompt}\n\n{user_prompt}\n\n### Response:\n"
raw_output = llm(prompt, stop=["###"], max_tokens=-1, temperature=1)
reply = raw_output.get("choices")[0].get("text").strip()
print(reply)
</code></pre>

<h3>Understanding the Parameters</h3>

<ul>
<li>stop=["###"]: Stops token generation at "###".</li>
<li>max_tokens: Sets a limit on the number of tokens. -1 for no limit.</li>
<li>temperature: Controls the randomness of output, ranging from 0 to 1.</li>
</ul>

<p>This module equips you with the know-how to set up and run a sarcastic chatbot using a Local Language Model. Feel free to modify the prompts and parameters as needed.</p>

<h3>Putting It All together: Marv the Sarcastic Bot</h3>

<pre><code>from llama_cpp import Llama

system_prompt = "You are Marv, a chatbot that reluctantly answers " \
                "questions with sarcastic responses."

user_prompt = "Hi Marv, what's up?"

prompt = f"### Instruction: {system_prompt}\n\n" \
         f"{user_prompt}\n\n" \
         f"### Response:\n"

llm = Llama(model_path="./app/models/openorca-platypus2-13b.Q4_K_M.gguf")

raw_output = llm(
    prompt,
    stop=["###"],
    max_tokens=-1,
    temperature=1,
)

reply = raw_output.get("choices")[0].get("text").strip()
print(reply)
</code></pre>

<h2>Additional Resources</h2>

<ul>
<li><a href="https://huggingface.co/models?pipeline_tag=text-generation&sort=trending">Huggingface Text Generation Models</a></li>
</ul>
