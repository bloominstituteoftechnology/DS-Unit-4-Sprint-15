# Local LLM setup

## Outline

1. Install Dependencies
2. Importing the Library: Imports the Llama class from the llama_cpp package. 
3. Defining Prompts:
   - system_prompt sets the persona of the chatbot.
   - user_prompt contains a user's query or statement.
   - prompt is the full prompt to be passed to the LLM, including the instruction and the user's input. The format of this prompt can be very important, different models may require a different format.
4. Initialize LLM: Initializes the local language model by providing the path to the model in Llama(model_path=...). 
5. Generate Response: Calls the LLM with the prompt, along with various parameters like stop, max_tokens, and temperature. It saves the output to raw_output. 
6. Extract and Print Reply: Extracts the generated reply from the raw_output and prints it.

### Install Dependencies

```bash
pip install llama-cpp-python
```

### Marv the Sarcastic Bot

```python
from llama_cpp import Llama


system_prompt = "You are Marv, a chatbot that reluctantly answers " \
                "questions with sarcastic responses."

user_prompt = "In the coming months AI will"

prompt = f"### Instruction: {system_prompt}\n\n" \
         f"{user_prompt}\n\n" \
         f"### Response:\n"

llm = Llama(model_path="./app/models/openorca-platypus2-13b.Q4_K_M.gguf")

raw_output = llm(
    prompt,
    stop=["###"],
    max_tokens=-1,
    temperature=1,
)

reply = raw_output.get("choices")[0].get("text").strip()
print(reply)
```

1. **System Prompt**: It serves as the instruction for the model, defining its persona. In this example, the persona is "Marv, a chatbot that reluctantly answers questions with sarcastic responses." This instructs the model to generate replies that are sarcastic in nature. The system prompt is often crucial in setting the tone, style, and context for how the language model should behave.

2. **User Prompt**: This is the query or statement from the user. The language model takes this as the actual question or issue to respond to. In your example, the user prompt is "In the coming months AI will," which would be a starting point for the model to generate a continuation.

3. **Composite Prompt**: Both the system and user prompts are combined to create a composite prompt (`prompt`). This composite prompt will be passed to the LLM. The structure is important; by preceding the user prompt with the system prompt and the instruction, the model is "primed" to answer in the persona set by the system prompt. 
   - For instance, if the user prompt is "tell me a joke," and the system prompt sets the persona as sarcastic, the generated reply might not be a straightforward joke but rather a sarcastic comment about telling jokes. 
   - The `### Instruction: {system_prompt}` and `### Response:` parts act as markers, helping the model to understand the roles of the system prompt and where its reply should start, respectively. This is particularly useful in fine-tuned models that have been trained to recognize such structural cues.

4. `stop=["###"]`: Stops generating tokens when it encounters "###". This is not strictly necessary but helps to keep the model from talking to itself in an endless loop. Some models are more prone to this issue than others.

5. `max_tokens`: No limit on the number of tokens to be generated when set to -1, otherwise this controls the maximum output by truncation if necessary.

6. `temperature`: Controls the randomness of the output (lower means less random). Ranges from 0 (deterministic) to 1 (non-deterministic). For many models this setting is more of a suggestion than a directive.

### Challenge

1. Experiment with the various prompts and parameters, see what you can build!
2. Refactor your bot into a function or class. At minimum parameterize the `user_prompt`.
3. (Stretch Goal) Implement a short term memory model for your bot. It can be as simple as feeding the previous interactions back into the LLM, or as complicated as a vector database with automatic relevant recall.
