# Local LLM setup

## Introduction
This module outlines the setup and usage of a Local Language Model (LLM) to create a chatbot named Marv, who is programmed to provide sarcastic responses. The LLM is powered by the llama_cpp Python package and is fine-tuned to answer queries based on the persona set by the system prompt.

### Topics Covered
- Installing Dependencies
- Initializing the LLM
- Crafting System and User Prompts
- Running the LLM and Obtaining a Response
- Understanding the Parameters

### Installing Dependencies

To get started, install the llama-cpp-python package using pip.

```bash
pip install llama-cpp-python
```

### Download the LLM

- [openorca-platypus2](https://huggingface.co/TheBloke/OpenOrca-Platypus2-13B-GGUF/resolve/main/openorca-platypus2-13b.Q4_K_M.gguf)

### Initializing the LLM

Import the Llama class and initialize it with the appropriate model path.

```python
from llama_cpp import Llama
llm = Llama(model_path="./app/models/openorca-platypus2-13b.Q4_K_M.gguf")
```

### Crafting System and User Prompts

Set up the system and user prompts. The system prompt acts as the instruction for the LLM, specifying its persona. The user prompt serves as the query or statement from the user.

1. **System Prompt**: It serves as the instruction for the model, defining its persona. In this example, the persona is "Marv, a chatbot that reluctantly answers questions with sarcastic responses." This instructs the model to generate replies that are sarcastic in nature. The system prompt is often crucial in setting the tone, style, and context for how the language model should behave.

2. **User Prompt**: This is the query or statement from the user. The language model takes this as the actual question or issue to respond to. In your example, the user prompt is "In the coming months AI will," which would be a starting point for the model to generate a continuation.

```python
system_prompt = "You are Marv, a chatbot that reluctantly answers questions with sarcastic responses."
user_prompt = "Hi Marv, what's up?"

```

### Running the LLM and Obtaining a Response

Create a composite prompt by combining the system and user prompts and run the LLM. Extract and print the response.

```python
prompt = f"### Instruction: {system_prompt}\n\n{user_prompt}\n\n### Response:\n"
raw_output = llm(prompt, stop=["###"], max_tokens=-1, temperature=1)
reply = raw_output.get("choices")[0].get("text").strip()
print(reply)
```

### Understanding the Parameters

- stop=["###"]: Stops token generation at "###".
- max_tokens: Sets a limit on the number of tokens. -1 for no limit.
- temperature: Controls the randomness of output, ranging from 0 to 1.

This module equips you with the know-how to set up and run a sarcastic chatbot using a Local Language Model. Feel free to modify the prompts and parameters as needed.


### Putting It All together: Marv the Sarcastic Bot

```python
from llama_cpp import Llama

system_prompt = "You are Marv, a chatbot that reluctantly answers " \
                "questions with sarcastic responses."

user_prompt = "Hi Marv, what's up?"

prompt = f"### Instruction: {system_prompt}\n\n" \
         f"{user_prompt}\n\n" \
         f"### Response:\n"

llm = Llama(model_path="./app/models/openorca-platypus2-13b.Q4_K_M.gguf")

raw_output = llm(
    prompt,
    stop=["###"],
    max_tokens=-1,
    temperature=1,
)

reply = raw_output.get("choices")[0].get("text").strip()
print(reply)
```

## Additional Resources
- [Huggingface Text Generation Models](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending)
